{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1281cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import dlib\n",
    "import imutils\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51db555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining prototext and caffemodel paths\n",
    "\n",
    "caffeModel = \"/Users/chauhanvarnika/Documents/Py-OpenCV/FaceDetection/Lip gesture Project/model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "prototextPath = \"/Users/chauhanvarnika/Documents/Py-OpenCV/FaceDetection/Lip gesture Project/model/deploy.prototxt.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff05cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dlib landmark detection path \n",
    "\n",
    "detFile = \"/Users/chauhanvarnika/Documents/Py-OpenCV/FaceDetection/Lip gesture Project/model/shape_predictor_68_face_landmarks.dat\"\n",
    "getLandmark = dlib.shape_predictor(detFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3b42df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...................\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "print(\"Loading model...................\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototextPath,caffeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567e3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPoints(image, faceLandmarks, startpoint, endpoint, isClosed=True):\n",
    "  for i in range(startpoint, endpoint+1):\n",
    "    \n",
    "    #For mouth open and close gesture\n",
    "    lip_left_coord = (int (faceLandmarks[60][0]), int(faceLandmarks[60][1]))\n",
    "    lip_right_coord = (int (faceLandmarks[64][0]), int(faceLandmarks[64][1]))\n",
    "    lip_wd = (lip_left_coord[0] - lip_right_coord[0]) ** 2 +( lip_left_coord[1] - lip_right_coord[1]) ** 2 \n",
    "\n",
    "    lip_top_coord = (int (faceLandmarks[62][0]), int(faceLandmarks[62][1]))\n",
    "    lip_btm_coord = (int (faceLandmarks[66][0]), int(faceLandmarks[66][1]))\n",
    "    lip_ht = (lip_top_coord[0] - lip_btm_coord[0]) ** 2 +( lip_top_coord[1] - lip_btm_coord[1]) ** 2\n",
    "    \n",
    "    #adding aspect ratio threshold for mouth open and close state\n",
    "    aspect_ratio = round((lip_ht/lip_wd), 2)\n",
    "    if aspect_ratio > 0.08:                                                                             \n",
    "        cv2.putText(image, \"open mouth\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (10,0,255))\n",
    "    if aspect_ratio == 0.0:                                                                             \n",
    "        cv2.putText(image, \"close mouth\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (10,0,255))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3e15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceEmbedding(image, detections):\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter detections by confidence greater than the minimum confidence\n",
    "        if confidence < 0.5 :\n",
    "            continue\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        dlib_rectangle = dlib.rectangle(left=startX, top=startY, right=endX, bottom=endY)\n",
    "        \n",
    "        #Adding confidence to the image\n",
    "        #text = \"{:.2f}%\".format(confidence * 100)\n",
    "        #y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        #cv2.putText(frame, text, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        #Getting landmarks by passing the frame to the landmark detector\n",
    "        shape = getLandmark(image, dlib_rectangle)  \n",
    "        \n",
    "        #Creating a 68 X 2 numpy array to store (x,y) coordiates of each landmark.\n",
    "        shape_np = np.zeros((68, 2), dtype=\"int\")                                                       \n",
    "        for i in range(0, 68):\n",
    "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        shape = shape_np\n",
    "\n",
    "        # Display the landmarks\n",
    "        for i, (x, y) in enumerate(shape):\n",
    "            if i == 47 :\n",
    "                drawPoints(image, shape, 48, 59)\n",
    "            elif i == 59:\n",
    "                drawPoints(image, shape, 60, 67)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9c5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream to get the live video frames\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame,width=400)\n",
    "\n",
    "    # extract the dimensions , Resize image into 300x300 and converting image into blobFromImage\n",
    "    (h, w) = frame.shape[:2]\n",
    "    # blobImage convert RGB (104.0, 177.0, 123.0)\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    "   #passing blob through the network to detect and pridiction\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    getFaceEmbedding(frame, detections)\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow('Lip Gesture Detection', frame)\n",
    "\n",
    "    # escape button to terminate the code\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89992d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
